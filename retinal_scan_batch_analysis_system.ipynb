{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "17c5b870",
      "metadata": {
        "id": "17c5b870"
      },
      "source": [
        "### Project: High-Throughput System for Batch Analysis of Retinal Scans\n",
        "\n",
        "Here is a comprehensive Google Colab notebook structure designed to address your project requirements. This notebook will guide you through training a deep learning model to classify retinal images and then using that model to perform batch analysis on a directory of new images.\n",
        "\n",
        "The \"Ocular Disease Recognition\" (ODIR) dataset from Kaggle is a suitable choice for this task. It contains 5,000 patient cases with fundus photographs of both left and right eyes, annotated with eight different labels, including Normal, Diabetes, and Glaucoma.\n",
        "\n",
        "**Note on the ODIR Dataset:** This dataset presents some challenges. It is highly imbalanced, with a large number of \"Normal\" images compared to specific diseases. Additionally, the primary labels correspond to the patient, not necessarily individual eye images, which can lead to complexities in training. For the purpose of this notebook, we will focus on a simplified three-class problem: Normal, Diabetic Retinopathy, and Glaucoma.\n",
        "\n",
        "---\n",
        "\n",
        "### Google Colab Notebook\n",
        "\n",
        "#### 1. Setup\n",
        "\n",
        "This section installs necessary libraries and imports the required modules for the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a8b52451",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8b52451",
        "outputId": "39b65e40-daab-40a8-964f-9af29fdf6c01",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (from opendatasets) (1.7.4.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from opendatasets) (8.2.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow pandas numpy scikit-learn matplotlib seaborn opendatasets\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "import opendatasets as od"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eb8410a",
      "metadata": {
        "id": "3eb8410a"
      },
      "source": [
        "#### 2. Data Ingestion & Preprocessing\n",
        "\n",
        "Here, we download the \"Ocular Disease Recognition\" dataset from Kaggle and preprocess it for our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c2767516",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2767516",
        "outputId": "678e32ec-fa30-423c-f95f-23d87d76072e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping, found downloaded files in \"./ocular-disease-recognition-odir5k\" (use force=True to force download)\n",
            "Found 5600 validated image filenames.\n",
            "Found 1400 validated image filenames.\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset from Kaggle\n",
        "od.download(\"https://www.kaggle.com/andrewmvd/ocular-disease-recognition-odir5k\")\n",
        "\n",
        "# Path to the dataset\n",
        "data_dir = './ocular-disease-recognition-odir5k/ODIR-5K/ODIR-5K/Training Images'\n",
        "\n",
        "# Load the labels\n",
        "labels_path = './ocular-disease-recognition-odir5k/ODIR-5K/ODIR-5K/data.xlsx' # Corrected path\n",
        "labels_df = pd.read_excel(labels_path)\n",
        "\n",
        "# Define the classes based on dataset documentation/common pathologies\n",
        "# N, D, G, C, A, H, M, O\n",
        "classes = [\n",
        "    'Normal',\n",
        "    'Diabetes',\n",
        "    'Glaucoma',\n",
        "    'Cataract',\n",
        "    'Age related Macular Degeneration',\n",
        "    'Hypertension',\n",
        "    'Pathological Myopia',\n",
        "    'Other diseases/abnormalities'\n",
        "]\n",
        "\n",
        "# Function to parse keywords and create one-hot encoded labels\n",
        "def get_labels(keywords):\n",
        "    labels = {cls: 0 for cls in classes}\n",
        "    if 'normal' in keywords:\n",
        "        labels['Normal'] = 1\n",
        "    if 'diabetic retinopathy' in keywords:\n",
        "        labels['Diabetes'] = 1\n",
        "    if 'glaucoma' in keywords:\n",
        "        labels['Glaucoma'] = 1\n",
        "    if 'cataract' in keywords:\n",
        "        labels['Cataract'] = 1\n",
        "    if 'age-related macular degeneration' in keywords:\n",
        "        labels['Age related Macular Degeneration'] = 1\n",
        "    if 'hypertensive retinopathy' in keywords:\n",
        "        labels['Hypertension'] = 1\n",
        "    if 'pathological myopia' in keywords:\n",
        "        labels['Pathological Myopia'] = 1\n",
        "    if 'other' in keywords: # Simplified check for 'other'\n",
        "         labels['Other diseases/abnormalities'] = 1\n",
        "    return list(labels.values())\n",
        "\n",
        "# Create a list of image records with multi-label format\n",
        "image_records = []\n",
        "for _, row in labels_df.iterrows():\n",
        "    left_labels = get_labels(row['Left-Diagnostic Keywords'].lower())\n",
        "    right_labels = get_labels(row['Right-Diagnostic Keywords'].lower())\n",
        "    image_records.append({'filename': row['Left-Fundus'], **dict(zip(classes, left_labels))})\n",
        "    image_records.append({'filename': row['Right-Fundus'], **dict(zip(classes, right_labels))})\n",
        "\n",
        "# Create the final DataFrame\n",
        "image_df = pd.DataFrame(image_records)\n",
        "\n",
        "# Data augmentation and splitting\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2, # 80% training, 20% validation\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=image_df,\n",
        "    directory=data_dir,\n",
        "    x_col=\"filename\",\n",
        "    y_col=classes,\n",
        "    subset=\"training\",\n",
        "    batch_size=32,\n",
        "    seed=42,\n",
        "    shuffle=True,\n",
        "    class_mode=\"raw\", # Use \"raw\" for multi-label\n",
        "    target_size=(224, 224))\n",
        "\n",
        "validation_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=image_df,\n",
        "    directory=data_dir,\n",
        "    x_col=\"filename\",\n",
        "    y_col=classes,\n",
        "    subset=\"validation\",\n",
        "    batch_size=32,\n",
        "    seed=42,\n",
        "    shuffle=True,\n",
        "    class_mode=\"raw\", # Use \"raw\" for multi-label\n",
        "    target_size=(224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56628616",
      "metadata": {
        "id": "56628616"
      },
      "source": [
        "#### 3. Model Architecture\n",
        "\n",
        "We will use a pre-trained ResNet50 model and add a custom classification head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "da39ad76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da39ad76",
        "outputId": "1a799495-ee8b-4062-8ad0-6284d47ab465"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# --- Replacement for Cell 3: Model Architecture ---\n",
        "\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "# The output layer now has 8 neurons (one for each class) and uses sigmoid activation\n",
        "predictions = Dense(len(classes), activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12afa540",
      "metadata": {
        "id": "12afa540"
      },
      "source": [
        "#### 4. Model Training\n",
        "\n",
        "This section compiles and trains the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa6fb1df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa6fb1df",
        "outputId": "cace1080-a0bb-42f3-bbed-6adeda23a6ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.7129 - auc_1: 0.4394 - loss: 0.3029"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1702s\u001b[0m 10s/step - accuracy: 0.7132 - auc_1: 0.4393 - loss: 0.3026 - val_accuracy: 0.8207 - val_auc_1: 0.4344 - val_loss: 0.2063 - learning_rate: 1.0000e-04\n",
            "Epoch 2/4\n"
          ]
        }
      ],
      "source": [
        "# Use binary_crossentropy for multi-label classification\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(multi_label=True)])\n",
        "\n",
        "# Callbacks\n",
        "checkpoint = ModelCheckpoint(\"retinal_model_multilabel.h5\", save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=4,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=[checkpoint, early_stopping, reduce_lr]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "766390e8",
      "metadata": {
        "id": "766390e8"
      },
      "source": [
        "#### 5. Performance Evaluation\n",
        "\n",
        "Evaluate the model's performance on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "874a48cd",
      "metadata": {
        "id": "874a48cd"
      },
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix and Classification Report\n",
        "Y_pred = model.predict(validation_generator)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "print('Confusion Matrix')\n",
        "cm = confusion_matrix(validation_generator.classes, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.show()\n",
        "\n",
        "print('Classification Report')\n",
        "target_names = list(train_generator.class_indices.keys())\n",
        "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0f5a027",
      "metadata": {
        "id": "f0f5a027"
      },
      "source": [
        "#### 6. Batch Processing & Inference\n",
        "\n",
        "This function takes a directory of new images and generates predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "231fcdd5",
      "metadata": {
        "id": "231fcdd5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "def batch_predict(image_dir, model_path, class_labels):\n",
        "    model = load_model(model_path)\n",
        "    image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "    results = []\n",
        "\n",
        "    for filename in image_files:\n",
        "        img_path = os.path.join(image_dir, filename)\n",
        "        img = image.load_img(img_path, target_size=(224, 224))\n",
        "        img_array = image.img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
        "\n",
        "        predictions = model.predict(img_array)[0]\n",
        "        result_dict = {'filename': filename}\n",
        "        # Create a dictionary mapping class label to its predicted probability\n",
        "        for label, prob in zip(class_labels, predictions):\n",
        "            result_dict[f'{label}_prob'] = prob\n",
        "        results.append(result_dict)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Example usage:\n",
        "# !mkdir -p new_images\n",
        "# # (add some images to this directory)\n",
        "# batch_results = batch_predict('new_images', 'retinal_model_multilabel.h5', classes)\n",
        "# print(batch_results.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07dc5900",
      "metadata": {
        "id": "07dc5900"
      },
      "source": [
        "#### 7. Results Aggregation & Reporting\n",
        "\n",
        "This final section provides summary statistics and visualizations of the batch predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5bc8e1d",
      "metadata": {
        "id": "b5bc8e1d"
      },
      "outputs": [],
      "source": [
        "def analyze_batch_results_multilabel(df, class_labels, threshold=0.5):\n",
        "    prob_cols = [f'{label}_prob' for label in class_labels]\n",
        "\n",
        "    # Identify all predicted labels for each image based on the threshold\n",
        "    predicted_labels = df[prob_cols].gt(threshold)\n",
        "    predicted_labels.columns = class_labels\n",
        "\n",
        "    # Concatenate results for analysis\n",
        "    analysis_df = pd.concat([df[['filename']], predicted_labels], axis=1)\n",
        "\n",
        "    # Generate Summary Statistics\n",
        "    summary_stats = analysis_df[class_labels].sum()\n",
        "    print(\"Summary Statistics (Total Counts):\")\n",
        "    print(summary_stats)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Create Visualizations\n",
        "    summary_stats.plot(kind='bar', title='Distribution of Predicted Conditions (Batch Summary)')\n",
        "    plt.ylabel('Total Detections')\n",
        "    plt.show()\n",
        "\n",
        "    # Prioritize High-Risk Cases\n",
        "    # A case is high-risk if any pathology (non-Normal) is detected\n",
        "    pathology_labels = [label for label in class_labels if label != 'Normal']\n",
        "    analysis_df['is_high_risk'] = analysis_df[pathology_labels].any(axis=1)\n",
        "\n",
        "    high_risk_df = analysis_df[analysis_df['is_high_risk']].copy()\n",
        "\n",
        "    # Find the highest pathology probability for sorting\n",
        "    pathology_prob_cols = [f'{label}_prob' for label in pathology_labels]\n",
        "    high_risk_df['max_risk_prob'] = df.loc[high_risk_df.index, pathology_prob_cols].max(axis=1)\n",
        "\n",
        "    # Get the list of predicted diseases for high-risk cases\n",
        "    def get_detected_diseases(row):\n",
        "        return [label for label in pathology_labels if row[label]]\n",
        "    high_risk_df['detected_diseases'] = high_risk_df.apply(get_detected_diseases, axis=1)\n",
        "\n",
        "    high_risk_sorted = high_risk_df.sort_values(by='max_risk_prob', ascending=False)\n",
        "\n",
        "    print(\"High-Priority Cases (Any Pathology Detected):\")\n",
        "    print(high_risk_sorted[['filename', 'detected_diseases', 'max_risk_prob']])\n",
        "\n",
        "# Example usage with the results from the previous step:\n",
        "# analyze_batch_results_multilabel(batch_results, classes)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}